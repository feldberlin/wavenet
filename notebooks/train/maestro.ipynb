{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from wavenet import model, train, sample, audio, datasets, utils, viz, debug, distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Maestro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell contains papermill tagged parameters\n",
    "# they can be overriden by the cli when training:  \n",
    "# papermill in.ipynb out.ipynb -p batch_norm True\n",
    "\n",
    "batch_norm = False\n",
    "learning_rate = 0.0044\n",
    "finder = False\n",
    "batch_size = 12\n",
    "max_epochs = 2\n",
    "with_all_chans = None\n",
    "sample_overlap_receptive_field = True\n",
    "progress_bar = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.HParams(\n",
    "    embed_inputs=True, \n",
    "    n_audio_chans=1, \n",
    "    squash_to_mono=True,\n",
    "    sample_overlap_receptive_field=sample_overlap_receptive_field,\n",
    "    batch_norm=batch_norm\n",
    ")\n",
    "\n",
    "if with_all_chans:\n",
    "    p = p.with_all_chans(with_all_chans)\n",
    "\n",
    "pp.pprint(dict(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = train.HParams(\n",
    "    max_epochs=max_epochs, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=8, \n",
    "    finder=finder, \n",
    "    progress_bar=progress_bar,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "pp.pprint(dict(tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.seed(p)\n",
    "nas_path = Path('/srv/datasets/maestro/maestro-v2.0.0')\n",
    "ssd_path = Path('/srv/datasets-ssd/maestro/maestro-v2.0.0')\n",
    "ds_train, ds_test = datasets.maestro(nas_path, p, ssd_path, year=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "utils.seed(p)\n",
    "m = model.Wavenet(p)\n",
    "debug.summarize(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = distributed.DDP(m, ds_train, ds_test, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_i = viz.plot_random_track(ds_train)\n",
    "track, *_ = ds_train[track_i]\n",
    "ipd.Audio(audio.mu_expand(track.squeeze().numpy(), p), rate=p.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "utils.seed(p)\n",
    "t.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.seed(p)\n",
    "tracks, logits, g = sample.fast(m, ds_train.transforms, utils.decode_nucleus(), n_samples=32000, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for track in tracks:\n",
    "    track = ds_train.transforms.normalise(track.numpy())\n",
    "    track = audio.mu_expand(track, p)\n",
    "    ipd.display(ipd.Audio(track, rate=p.sampling_rate))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": ".venv-wavenet",
   "language": "python",
   "name": ".venv-wavenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
